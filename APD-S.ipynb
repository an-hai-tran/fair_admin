{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77bf07f",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030c858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import libraries for machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import libraries to solve LP\n",
    "from pulp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9258ce",
   "metadata": {},
   "source": [
    "# Output $p_i$ and $q_i$ for each candidate i $\\in$ [n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df3b15",
   "metadata": {},
   "source": [
    "We process the data and train the selected algorithm with the training data. After that, we output $p_i$ and $q_i$ on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2b3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('clean_law_school.csv', index_col = 0)\n",
    "\n",
    "# split data into training and testing part\n",
    "target = ['admit', 'enroll']\n",
    "y = df[target]\n",
    "X = df.drop(target, axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, shuffle = True, random_state = 1)\n",
    "\n",
    "# implement the machine learning model to predict p_i and q_i\n",
    "lg_clf = ClassifierChain(RandomForestClassifier())\n",
    "lg_clf.fit(X_train, y_train)\n",
    "y_pred = lg_clf.predict_proba(X_test)\n",
    "X_test = pd.merge(X_test, y_test, left_index = True, right_index = True)\n",
    "X_test[['p_i', 'q_i']] = np.round(y_pred.toarray(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324bf0d9",
   "metadata": {},
   "source": [
    "In APD-S, since we assume there is only one academic unit (e.g, department) and one admission committe (AC), we would only select applicants from colleges with similar pass interview rate and accept offer rate for the input instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2de79bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22754.000000\n",
       "mean         0.347415\n",
       "std          0.238890\n",
       "min          0.000000\n",
       "25%          0.192000\n",
       "50%          0.315000\n",
       "75%          0.432000\n",
       "max          1.000000\n",
       "Name: cpir, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of college acceptance rate\n",
    "X_test['cpir'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412991b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22754.000000\n",
       "mean         0.356386\n",
       "std          0.237919\n",
       "min          0.000000\n",
       "25%          0.180000\n",
       "50%          0.285000\n",
       "75%          0.433000\n",
       "max          1.000000\n",
       "Name: caor, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of college admission rate\n",
    "X_test['caor'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee62fd",
   "metadata": {},
   "source": [
    "Based on the summary of the statistics of colleges' pass interview and accept offer rate, we would select applicants from college in the range of 25th percentile and 75th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7de1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[(X_test['caor'] >= np.percentile(X_test['caor'], 25)) & \\\n",
    "                (X_test['caor'] <= np.percentile(X_test['caor'], 75)) & \\\n",
    "                (X_test['cpir'] >= np.percentile(X_test['cpir'], 25)) & \\\n",
    "                (X_test['cpir'] <= np.percentile(X_test['cpir'], 75))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622475c5",
   "metadata": {},
   "source": [
    "# Generating input instance for APD-S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171608f9",
   "metadata": {},
   "source": [
    "An input instance of APD can be characterized as $I = ([n], \\{p_i, q_i |i \\in [n]\\}, \\{B_g, g|g \\in G_I\\}, \\{b_g, g|g \\in G_E\\}, \\{w_{ig}, \\tau_g|g \\in G_P, i \\in g\\})$.\n",
    "\n",
    "Note: \n",
    "- In this dataset, we assume all candidates are qualified for an interview and they will be automatically offered once passing it. Hence, it can be explained why we use the result of *admit* as *passing the interview* and *enroll* as *accepting the offer*.\n",
    "\n",
    "Input details:\n",
    "- [n] = 1000 candidates\n",
    "- $p_i, q_i$ for each i $\\in$ [n]\n",
    "- $G_I$:\n",
    "    - In APD-S, we only have single interview-related constraint that can be captured as {g, B} with g = [n] being the only group in $G_I$. So $G_I$ contains only 1 group with n candidates.\n",
    "    - In reality, the dataset should miss a lot of candidates that fail to get an interview since it only includes those who qualify for an interview. In this case, we can simulate how colleges with similar acceptance rate (~20-30%) as schools in this data perform. We found [Admissions report of Oxford Law](https://www.law.ox.ac.uk/sites/files/oxlaw/ug_admissions_report_2021.pdf) and identified that we can use its application-to-interview success rate to set the cap on interview-related group g $B_g$ accordingly.\n",
    "- $G_E$:\n",
    "    - We have enrollment-related budget constraints {g, $b_g|g \\in G_E$}. In this dataset, there are two groups inside $G_E$ which indicate candidates who are in-state and those who are out-of-state.\n",
    "    - Since the data should not miss any candidate who successfully enrolls, we can set the cap on enrollment-related group g $b_g$ as the actual statistics of the dataset (of those who enroll, who are in-state applicants, who are out-of-state applicants?)\n",
    "- $G_P$:\n",
    "    - We capture the collection of protected groups of interest $G_P$ as the combination of the race and gender of the candidates. That means $G_P$ will have 8 groups by combining Race: {Black, Hispanic, Asian, White} and Gender: {Male, Female}.\n",
    "    - We identify the target quota $\\tau_g$ for protected group g using the admission statistics of universities that are known for applying Affirmative Action in their admission process. \n",
    "        + Specifically, we would use the statistics of Harvard Law School, known for its [yearly commitment to Affirmative Action in the admission/employment process](https://hr.harvard.edu/files/humanresources/files/reaffirmation_statement.pdf)\n",
    "        + Based on the [demographics of Hardvard Fall 2020 applications](https://www.ilrg.com/rankings/law/view/49), we calculate the percentage of each protected group in the enrollment number and set $\\tau_g$ accordingly. \n",
    "    - For each candidate i $\\in g$ of $G_P, w_{ig}$ (the degree of relevance of i to g) is calculated by [XXX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d704dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 10 random input instances from the testing data set, each with 2000 applicants\n",
    "n = 1000\n",
    "test_data = [X_test.sample(n=n).reset_index(drop=True) for num in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b68a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_input(data):\n",
    "    # collection of interview-related groups\n",
    "    G_I = data\n",
    "    \n",
    "    # collection of enrollment-related groups\n",
    "    in_state = (data['resident'] == 1)\n",
    "    out_of_state = (data['resident'] == 0)\n",
    "    G_E = [data[in_state], data[out_of_state]]\n",
    "    \n",
    "    # collection of protected groups\n",
    "    female_black = data[(data['gender'] == 0) & (data['black']==1)]\n",
    "    female_hispanic = data[(data['gender'] == 0) & (data['hispanic'] == 1)]\n",
    "    female_asian = data[(data['gender'] == 0) & (data['asian'] == 1)]\n",
    "    female_white = data[(data['gender'] == 0) & (data['white'] == 1)]\n",
    "    female_other = data[(data['gender'] == 0) & (data['other_race'] == 1)]\n",
    "    \n",
    "    male_black = data[(data['gender'] == 1) & (data['black'] == 1)]\n",
    "    male_hispanic = data[(data['gender'] == 1) & (data['hispanic'] == 1)]\n",
    "    male_asian = data[(data['gender'] == 1) & (data['asian'] == 1)]\n",
    "    male_white = data[(data['gender'] == 1) & (data['white'] == 1)]\n",
    "    male_other = data[(data['gender'] == 1) & (data['other_race'] == 1)]\n",
    "\n",
    "    G_P = [female_black, female_hispanic, female_asian, female_white, female_other,\\\n",
    "           male_black, male_hispanic, male_asian, male_white, male_other] \n",
    "    \n",
    "    # cap imposed on interview-related group g\n",
    "    B_g = int(len(data) * 0.3705)\n",
    "    \n",
    "    # cap imposed on enrollment-related group g\n",
    "    b_g = [len(data[in_state & (data['enroll'] == 1)]), len(data[out_of_state & (data['enroll'] == 1)])]\n",
    "    \n",
    "    # target quota for protected group g to achieve\n",
    "    target_quota = [0.0345, 0.0415, 0.0535, 0.252, 0.1185] * 2\n",
    "    enroll = (data['enroll'] == 1)\n",
    "    tau_g = np.array(np.round([len(data[enroll]) * quota for quota in target_quota]), int)\n",
    "    \n",
    "    # if there is a difference in the sum of G_P and G_E due to rounding, randomly increase one group\n",
    "    # in either G_P or G_E to balance the difference (as both cap on final enrollment)\n",
    "    if sum(tau_g) > sum(b_g):\n",
    "        index = np.random.randint(0, len(b_g)) \n",
    "        b_g[index] += sum(tau_g) - sum(b_g)\n",
    "    elif sum(tau_g) < sum(b_g):\n",
    "            index = np.random.randint(0, len(tau_g))\n",
    "            tau_g[index] += sum(b_g) - sum(tau_g)\n",
    "    \n",
    "    # relevance of i to protected group g\n",
    "    w_ig = []\n",
    "    for index_g in range(len(G_P)):\n",
    "        g = G_P[index_g]\n",
    "        arr = []\n",
    "        for index_i in range(len(g)):\n",
    "            arr.append(g.iloc[index_i]['es'])\n",
    "        w_ig.append(arr)\n",
    "    \n",
    "    return G_I, G_E, G_P, B_g, b_g, tau_g, w_ig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b36a63",
   "metadata": {},
   "source": [
    "The objective model is max min$_{g \\in G_P} (\\sum_{i \\in g} w_{ig} y_i q_i / \\tau_g)$.\n",
    "\n",
    "We can rewrite it as the following to solve:\n",
    "\n",
    "max z\n",
    "\n",
    "s.t $ \\space \\space$  z $\\le \\sum_{i \\in g} w_{ig} y_i q_i / \\tau_g \\space \\space \\space \\space \\space \\space$ for $g \\in G_P$\n",
    "\n",
    "Other constraints will be kept as original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "360b0906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def solveLP(data):\n",
    "    # create input instance\n",
    "    G_I, G_E, G_P, B_g, b_g, tau_g, w_ig = generate_input(data)\n",
    "    \n",
    "    # create model\n",
    "    model = LpProblem(name='APD-S', sense = LpMaximize)\n",
    "\n",
    "    # define decision variables\n",
    "    x_name = []\n",
    "    y_name = []\n",
    "    for i in range(n):\n",
    "        x_name.append('x' + str(i))\n",
    "        y_name.append('y' + str(i))\n",
    "\n",
    "    x = [LpVariable(x_name[i], lowBound = 0, upBound = 1) for i in range(n)]\n",
    "    y = [LpVariable(y_name[i], lowBound = 0, upBound = 1) for i in range(n)]\n",
    "    z = LpVariable(name='z')\n",
    "\n",
    "    # add objective function to the model\n",
    "    model += z\n",
    "\n",
    "    # constraints for z\n",
    "    for index_g in range(len(G_P)):\n",
    "        constraint = []\n",
    "        g = G_P[index_g]\n",
    "        for index_i in range(len(g)):\n",
    "            constraint.append(w_ig[index_g][index_i]*y[g.index[index_i]]*g.iloc[index_i]['q_i']/tau_g[index_g])\n",
    "        model += z <= lpSum(constraint)\n",
    "    \n",
    "    # constraints for (2) in LP\n",
    "    constraint = []\n",
    "    for index_g in range(len(G_I)):\n",
    "        constraint.append(x[index_g])\n",
    "    model += lpSum(constraint) <= B_g\n",
    "    \n",
    "    # constraints for (3) in LP\n",
    "    for i in range(n):\n",
    "        model += y[i] <= x[i]*data.iloc[i]['p_i']\n",
    "        \n",
    "    # constraints for (4) in LP\n",
    "    for index_g in range(len(G_E)):\n",
    "        constraint = []\n",
    "        g = G_E[index_g]\n",
    "        for index_i in range(len(g)):\n",
    "            constraint.append(y[g.index[index_i]]*g.iloc[index_i]['q_i'])\n",
    "        model += lpSum(constraint) <= b_g[index_g]\n",
    "    \n",
    "    # solve the model \n",
    "    model.solve(PULP_CBC_CMD(msg=0))\n",
    "    \n",
    "    x_optimal = [0]*n\n",
    "    y_optimal = [0]*n\n",
    "\n",
    "    for var in model.variables()[:n]:\n",
    "        x_optimal[int(str(var.name)[1:])] = round(var.varValue, 3)\n",
    "    \n",
    "    for var in model.variables()[n:-1]:\n",
    "        y_optimal[int(str(var.name)[1:])] = round(var.varValue, 3)\n",
    "    \n",
    "    # return x*_i, y*_i for each candidate i\n",
    "    return x_optimal, y_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48be4738",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying all input instances\n",
      "-> Every constraints has been satisfied throughout 10 input instances\n"
     ]
    }
   ],
   "source": [
    "# verify if x* and y* of each input instance satisfy the constraints \n",
    "x_optimal = []\n",
    "y_optimal = []\n",
    "print('Verifying all input instances')\n",
    "for index in range(len(test_data)):\n",
    "    \n",
    "    x, y = solveLP(test_data[index])\n",
    "    x_optimal.append(x)\n",
    "    y_optimal.append(y)\n",
    "\n",
    "    G_I, G_E, G_P, B_g, b_g, tau_g, w_ig = generate_input(test_data[index])\n",
    "    flag = True\n",
    "    \n",
    "    # constraint 2\n",
    "    if sum(x_optimal[index]) > B_g:\n",
    "        flag = False\n",
    "\n",
    "    # constraint 3\n",
    "    for i in range(n):\n",
    "        if y_optimal[index][i] > x_optimal[index][i]*test_data[index].iloc[i]['p_i']:\n",
    "            flag = False\n",
    "    \n",
    "    # constraint 4\n",
    "    for index_g in range(len(G_E)):\n",
    "        g = G_E[index_g]\n",
    "        for index_i in range(len(g)):\n",
    "            if y_optimal[index][g.index[index_i]]*g.iloc[index_i]['q_i'] > b_g[index_g]:\n",
    "                flag = False\n",
    "if flag:\n",
    "    print('-> Every constraints has been satisfied throughout 10 input instances')\n",
    "else:\n",
    "    print('-> Not every constraints has been satisfied throughout 10 input instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628a65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_1(data):\n",
    "    x_optimal, y_optimal = solveLP(data)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
