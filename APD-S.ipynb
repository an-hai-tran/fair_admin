{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77bf07f",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030c858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import libraries for machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import libraries to solve LP\n",
    "from pulp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9258ce",
   "metadata": {},
   "source": [
    "# Output $p_i$ and $q_i$ for each candidate i $\\in$ [n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df3b15",
   "metadata": {},
   "source": [
    "We process the data and train the selected algorithm with the training data. After that, we output $p_i$ and $q_i$ on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('clean_law_school.csv', index_col = 0)\n",
    "\n",
    "# split data into training and testing part\n",
    "target = ['admit', 'enroll']\n",
    "y = df[target]\n",
    "X = df.drop(target, axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, shuffle = True, random_state = 1)\n",
    "\n",
    "# implement the machine learning model to predict p_i and q_i\n",
    "lg_clf = ClassifierChain(RandomForestClassifier())\n",
    "lg_clf.fit(X_train, y_train)\n",
    "y_pred = lg_clf.predict_proba(X_test)\n",
    "X_test = pd.merge(X_test, y_test, left_index = True, right_index = True)\n",
    "X_test[['p_i', 'q_i']] = np.round(y_pred.toarray(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324bf0d9",
   "metadata": {},
   "source": [
    "In APD-S, since we focus on one academic unit (e.g, department), we would only select applicants from colleges with similar admit rate and enroll rate (this means that we can form 1 department out of those schools) for the input instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of college admit rate\n",
    "X_test['c_admit_rate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412991b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of college enroll rate\n",
    "X_test['c_enroll_rate'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee62fd",
   "metadata": {},
   "source": [
    "Based on the summary of the statistics of colleges' pass interview and accept offer rate, we would select applicants from college in the range of 25th percentile and 75th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[(X_test['c_admit_rate'] >= np.percentile(X_test['c_admit_rate'], 25)) & \\\n",
    "                (X_test['c_admit_rate'] <= np.percentile(X_test['c_admit_rate'], 75)) & \\\n",
    "                (X_test['c_enroll_rate'] >= np.percentile(X_test['c_enroll_rate'], 25)) & \\\n",
    "                (X_test['c_enroll_rate'] <= np.percentile(X_test['c_enroll_rate'], 75))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622475c5",
   "metadata": {},
   "source": [
    "# Generating input instance for APD-S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171608f9",
   "metadata": {},
   "source": [
    "An input instance of APD can be characterized as $I = ([n], \\{p_i, q_i |i \\in [n]\\}, \\{B_g, g|g \\in G_I\\}, \\{b_g, g|g \\in G_E\\}, \\{w_{ig}, \\tau_g|g \\in G_P, i \\in g\\})$.\n",
    "\n",
    "Note: \n",
    "- In this dataset, we assume all candidates are qualified for an interview and they will be automatically offered once passing it. Hence, it can be explained why we use the result of *admit* as *passing the interview* and *enroll* as *accepting the offer*.\n",
    "\n",
    "Input details:\n",
    "- [n] $\\in$ {1000, 2000, 3000, 4000, 5000} candidates\n",
    "- $p_i, q_i$ for each i $\\in$ [n]\n",
    "- $G_I$:\n",
    "    - In APD-S, we only have single interview-related constraint that can be captured as {g, B} with g = [n] being the only group in $G_I$. So $G_I$ contains only 1 group with n candidates.\n",
    "    - In reality, the dataset should miss a lot of candidates that fail to get an interview since it only includes those who qualify for an interview. In this case, we can simulate how colleges with similar acceptance rate (~20-30%) as schools in this data perform. We found [Admissions report of Oxford Law](https://www.law.ox.ac.uk/sites/files/oxlaw/ug_admissions_report_2021.pdf) and identified that we can use its application-to-interview success rate to set the cap on interview-related group g $B_g$ accordingly.\n",
    "- $G_E$:\n",
    "    - We have enrollment-related budget constraints {g, $b_g|g \\in G_E$}. In this dataset, there are two groups inside $G_E$ which indicate candidates who are in-state and those who are out-of-state.\n",
    "    - Since the data should not miss any candidate who successfully enrolls, we can set the cap on enrollment-related group g $b_g$ as the actual statistics of the dataset (of those who enroll, who are in-state applicants, who are out-of-state applicants?)\n",
    "- $G_P$:\n",
    "    - We capture the collection of protected groups of interest $G_P$ as the combination of the race and gender of the candidates. That means $G_P$ will have 8 groups by combining Race: {Black, Hispanic, Asian, White} and Gender: {Male, Female}.\n",
    "    - We identify the target quota $\\tau_g$ for protected group g using the admission statistics of universities that are known for applying Affirmative Action in their admission process. \n",
    "        + Specifically, we would use the statistics of Harvard Law School, known for its [yearly commitment to Affirmative Action in the admission/employment process](https://hr.harvard.edu/files/humanresources/files/reaffirmation_statement.pdf)\n",
    "        + Based on the [demographics of Hardvard Fall 2020 applications](https://www.ilrg.com/rankings/law/view/49), we calculate the percentage of each protected group in the enrollment number and set $\\tau_g$ accordingly. \n",
    "    - For each candidate i $\\in g$ of $G_P, w_{ig}$ (the degree of relevance of i to g) is calculated by [XXX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 random input instances from the testing data set, each with {500, 1000, 1500, 2000, 2500} candidates\n",
    "sizes = np.array([n * 1000 for n in range(1, 6)])\n",
    "input_instances = np.array([X_test.sample(n = n).reset_index(drop = True) for n in sizes], dtype = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b68a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_input(data):\n",
    "    # collection of interview-related groups\n",
    "    G_I = data\n",
    "    \n",
    "    # collection of enrollment-related groups\n",
    "    in_state = (data['resident'] == 1)\n",
    "    out_of_state = (data['resident'] == 0)\n",
    "    G_E = np.array([data[in_state], data[out_of_state]], dtype = object)\n",
    "    \n",
    "    # collection of protected groups\n",
    "    female_black = data[(data['gender'] == 0) & (data['black']==1)]\n",
    "    female_hispanic = data[(data['gender'] == 0) & (data['hispanic'] == 1)]\n",
    "    female_asian = data[(data['gender'] == 0) & (data['asian'] == 1)]\n",
    "    female_white = data[(data['gender'] == 0) & (data['white'] == 1)]\n",
    "    female_other = data[(data['gender'] == 0) & (data['other_race'] == 1)]\n",
    "    \n",
    "    male_black = data[(data['gender'] == 1) & (data['black'] == 1)]\n",
    "    male_hispanic = data[(data['gender'] == 1) & (data['hispanic'] == 1)]\n",
    "    male_asian = data[(data['gender'] == 1) & (data['asian'] == 1)]\n",
    "    male_white = data[(data['gender'] == 1) & (data['white'] == 1)]\n",
    "    male_other = data[(data['gender'] == 1) & (data['other_race'] == 1)]\n",
    "\n",
    "    G_P = np.array([female_black, female_hispanic, female_asian, female_white, female_other,\\\n",
    "           male_black, male_hispanic, male_asian, male_white, male_other], dtype = object) \n",
    "    \n",
    "    # cap imposed on interview-related group g\n",
    "    B_g = int(len(data) * 0.3705)\n",
    "    \n",
    "    # cap imposed on enrollment-related group g\n",
    "    b_g = np.array([len(data[in_state & (data['enroll'] == 1)]), len(data[out_of_state & (data['enroll'] == 1)])])\n",
    "    \n",
    "    # target quota for protected group g to achieve\n",
    "    target_quota = [0.0345, 0.0415, 0.0535, 0.252, 0.1185] * 2\n",
    "    tau_g = np.array(np.round([len(data[data['enroll'] == 1]) * quota for quota in target_quota]), int)\n",
    "    \n",
    "    # if there is a difference in the sum of G_P and G_E due to rounding, randomly increase one group\n",
    "    # in either G_P or G_E to balance the difference (as both cap on final enrollment)\n",
    "    if sum(tau_g) > sum(b_g):\n",
    "        index = np.random.randint(0, len(b_g)) \n",
    "        b_g[index] += sum(tau_g) - sum(b_g)\n",
    "    elif sum(tau_g) < sum(b_g):\n",
    "            index = np.random.randint(0, len(tau_g))\n",
    "            tau_g[index] += sum(b_g) - sum(tau_g)\n",
    "    \n",
    "    # relevance of i to protected group g\n",
    "    w_ig = []\n",
    "    for index_g in range(len(G_P)):\n",
    "        g = G_P[index_g]\n",
    "        arr = []\n",
    "        for index_i in range(len(g)):\n",
    "            arr.append(g.iloc[index_i]['es'])\n",
    "        w_ig.append(arr)\n",
    "    w_ig = np.asarray(w_ig, dtype = object)\n",
    "    \n",
    "    return G_I, G_E, G_P, B_g, b_g, tau_g, w_ig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d919555",
   "metadata": {},
   "source": [
    "# Solve Linear Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b36a63",
   "metadata": {},
   "source": [
    "The objective model is max min$_{g \\in G_P} (\\sum_{i \\in g} w_{ig} y_i q_i / \\tau_g)$.\n",
    "\n",
    "We can rewrite it as the following to solve:\n",
    "\n",
    "max z\n",
    "\n",
    "s.t $ \\space \\space$  z $\\le \\sum_{i \\in g} w_{ig} y_i q_i / \\tau_g \\space \\space \\space \\space \\space \\space$ for $g \\in G_P$\n",
    "\n",
    "Other constraints will be kept as original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd15f87",
   "metadata": {},
   "source": [
    "### Function to solve LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b0906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def solveLP(data, n):\n",
    "    # create input instance\n",
    "    G_I, G_E, G_P, B_g, b_g, tau_g, w_ig = generate_input(data)\n",
    "    \n",
    "    # create model\n",
    "    model = LpProblem(name='APD-S', sense = LpMaximize)\n",
    "\n",
    "    # define decision variables\n",
    "    x = np.array([LpVariable('x' + str(i), lowBound = 0, upBound = 1) for i in range(n)])\n",
    "    y = np.array([LpVariable('y' + str(i), lowBound = 0, upBound = 1) for i in range(n)])\n",
    "    z = LpVariable(name='z')\n",
    "\n",
    "    # add objective function to the model\n",
    "    model += z\n",
    "\n",
    "    # constraints for z\n",
    "    for index_g in range(len(G_P)):\n",
    "        constraint = []\n",
    "        g = G_P[index_g]\n",
    "        for index_i in range(len(g)):\n",
    "            constraint.append(w_ig[index_g][index_i]*y[g.index[index_i]]*g.iloc[index_i]['q_i']/tau_g[index_g])\n",
    "        model += z <= lpSum(constraint)\n",
    "    \n",
    "    # constraints for (2) in LP\n",
    "    constraint = []\n",
    "    for index_g in range(len(G_I)):\n",
    "        constraint.append(x[index_g])\n",
    "    model += lpSum(constraint) <= B_g\n",
    "    \n",
    "    # constraints for (3) in LP\n",
    "    for i in range(n):\n",
    "        model += y[i] <= x[i]*data.iloc[i]['p_i']\n",
    "        \n",
    "    # constraints for (4) in LP\n",
    "    for index_g in range(len(G_E)):\n",
    "        g = G_E[index_g]\n",
    "        constraint = []\n",
    "        for index_i in range(len(g)):\n",
    "            constraint.append(y[g.index[index_i]]*g.iloc[index_i]['q_i'])\n",
    "        model += lpSum(constraint) <= b_g[index_g]\n",
    "    \n",
    "    # solve the model \n",
    "    model.solve(PULP_CBC_CMD(msg=0))\n",
    "    \n",
    "    x_optimal = np.zeros(n)\n",
    "    y_optimal = np.zeros(n)\n",
    "    \n",
    "    for var in model.variables()[:n]:\n",
    "        x_optimal[int(str(var.name)[1:])] = round(var.varValue, 3)\n",
    "    \n",
    "    for var in model.variables()[n:-1]:\n",
    "        y_optimal[int(str(var.name)[1:])] = round(var.varValue, 3)\n",
    "    \n",
    "    # return x*_i, y*_i for each candidate i\n",
    "    return x_optimal, y_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58360402",
   "metadata": {},
   "source": [
    "### Function to verify the constraints of LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be4738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verify if x* and y* of each input instance satisfy the constraints \n",
    "def verify_LP(data, n):\n",
    "    print('Verify input instance with n =', n)\n",
    "    \n",
    "    G_I, G_E, G_P, B_g, b_g, tau_g, w_ig = generate_input(data)\n",
    "    x_optimal, y_optimal = solveLP(data, n)\n",
    "\n",
    "    # constraint 2\n",
    "    if sum(x_optimal) > B_g:\n",
    "        print('Not satisfied!')\n",
    "        return False\n",
    "\n",
    "    # constraint 3\n",
    "    for i in range(n):\n",
    "        if y_optimal[i] > x_optimal[i]*data.iloc[i]['p_i']:\n",
    "            print('Not satisfied!')\n",
    "            return False\n",
    "\n",
    "    # constraint 4\n",
    "    for index_g in range(len(G_E)):\n",
    "        g = G_E[index_g]\n",
    "        for index_i in range(len(g)):\n",
    "            if y_optimal[g.index[index_i]]*g.iloc[index_i]['q_i'] > b_g[index_g]:\n",
    "                print('Not satisfied!')\n",
    "                return False\n",
    "            \n",
    "    # if no False\n",
    "    print('-> Every constraints has been satisfied!')\n",
    "\n",
    "for i in range(len(sizes)):\n",
    "    verify_LP(input_instances[i], sizes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89d173",
   "metadata": {},
   "source": [
    "# Implementation of algorithm 1 for the special case of APD-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_1(data, n):\n",
    "    G_I, G_E, G_P, B_g, b_g, tau_g, w_ig = generate_input(data)\n",
    "    B = B_g\n",
    "\n",
    "    # Step (1)\n",
    "    x_optimal, y_optimal = solveLP(data, n)\n",
    "\n",
    "    # Remarks on RP for APD-S\n",
    "    L_i = np.empty(n, dtype = object)\n",
    "    T_l = np.empty([n, B])\n",
    "    T_l_prime = np.empty([n, B])\n",
    "    for i in range(n):\n",
    "        # generate interval L_i for each i\n",
    "        left = sum(x_optimal[:i])\n",
    "        right = sum(x_optimal[:(i+1)])\n",
    "        L_i[i] = pd.Interval(left = left, right = right, closed = 'both')\n",
    "\n",
    "        # generate B i.i.d T_l and T'_l for each l\n",
    "        l = i\n",
    "        T_l[l] = np.random.uniform(0, np.nextafter(1, np.inf), B)\n",
    "        T_l_prime[l] = np.array([l - 1 + T for T in T_l[l]])\n",
    "\n",
    "    # set I_i = 1 if there exists at least one l in n with T'_l in L_i\n",
    "    I_i = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        exist = False\n",
    "        for l in range(n):\n",
    "            if all(num in L_i[i] for num in T_l_prime[l]): # if T'_l in L_i\n",
    "                I_i[i] = 1\n",
    "                break\n",
    "\n",
    "    # step (5) - (11):\n",
    "    pi = np.random.permutation(n)\n",
    "    Z_i = np.zeros(n)\n",
    "    s0 = 0\n",
    "    s1 = 0\n",
    "    for l in range(n):\n",
    "        i = pi[l]\n",
    "        if I_i[i] == 1: # give interview to i\n",
    "            p_i = data.iloc[i]['p_i']\n",
    "            if np.random.choice([True, False], size = 1, p = [p_i, 1 - p_i]): # if i passes the interview\n",
    "                O_i =  np.random.binomial(1, y_optimal[i]/(x_optimal[i] * p_i))\n",
    "                if O_i == 1: \n",
    "                    # locate i in which group g of G_E\n",
    "                    if i in G_E[0].index: \n",
    "                        if s0 <= b_g[0]:\n",
    "                            s0 += 1\n",
    "                            Z_i[i] = 1         \n",
    "                    elif i in G_E[1].index:\n",
    "                        if s1 <= b_g[1]:\n",
    "                            s1 += 1\n",
    "                            Z_i[i] = 1    \n",
    "    return Z_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf05e2a",
   "metadata": {},
   "source": [
    "# Verify Lemma 2: $E[Z_i] \\ge (y_i^{*}.q_i)/2$ for every $i \\in n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da0842",
   "metadata": {},
   "source": [
    "### Function to verify Lemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceaf067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def verify_lemma2(data, n):\n",
    "    print('Verify Lemma 2 with input instance that has n =', n)\n",
    "    expected_Z = np.zeros(n)\n",
    "    for i in range(100):\n",
    "        Z_i = algorithm_1(data, n)\n",
    "        expected_Z += Z_i\n",
    "    expected_Z /= 100\n",
    "    x_optimal, y_optimal = solveLP(data, n)\n",
    "    for i in range(n):\n",
    "        if expected_Z[i] < (y_optimal[i]*data.iloc[i]['q_i'])/2:\n",
    "            print(expected_Z[i])\n",
    "            print((y_optimal[i]*data.iloc[i]['q_i'])/2)\n",
    "            print('Lemma 2 has not been verified!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45a068",
   "metadata": {},
   "source": [
    "### Verify Lemma 2 with all input instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554643a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_lemma2(input_instances[0], sizes[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
