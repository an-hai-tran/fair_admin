{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77bf07f",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030c858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import libraries for machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import libraries to solve LP\n",
    "from pulp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9258ce",
   "metadata": {},
   "source": [
    "# Output $p_i$ and $q_i$ for each candidate i $\\in$ [n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924e6db",
   "metadata": {},
   "source": [
    "### Train the predict algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2b3ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=RandomForestClassifier(), require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=RandomForestClassifier(), require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=RandomForestClassifier(), require_dense=[True, True])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv('clean_law_school.csv', index_col = 0)\n",
    "\n",
    "# split data into training and testing part\n",
    "target = ['admit', 'enroll']\n",
    "y = df[target]\n",
    "X = df.drop(target, axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, shuffle = True, random_state = 1)\n",
    "\n",
    "# implement the machine learning model to predict p_i and q_i\n",
    "lg_clf = ClassifierChain(RandomForestClassifier())\n",
    "lg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c727d4",
   "metadata": {},
   "source": [
    "### Output $p_i$ and $q_i$ for each i in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a55856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lsat</th>\n",
       "      <th>gpa</th>\n",
       "      <th>resident</th>\n",
       "      <th>gender</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>asian</th>\n",
       "      <th>white</th>\n",
       "      <th>other_race</th>\n",
       "      <th>c_admit_rate</th>\n",
       "      <th>c_enroll_rate</th>\n",
       "      <th>es</th>\n",
       "      <th>admit</th>\n",
       "      <th>enroll</th>\n",
       "      <th>p_i</th>\n",
       "      <th>q_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54066</th>\n",
       "      <td>150.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59441</th>\n",
       "      <td>155.0</td>\n",
       "      <td>2.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11354</th>\n",
       "      <td>161.0</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.108</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68562</th>\n",
       "      <td>164.0</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.387</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97741</th>\n",
       "      <td>149.0</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51882</th>\n",
       "      <td>153.0</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62536</th>\n",
       "      <td>162.0</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98995</th>\n",
       "      <td>156.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6304</th>\n",
       "      <td>152.0</td>\n",
       "      <td>3.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62481</th>\n",
       "      <td>153.0</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22754 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lsat   gpa  resident  gender  black  hispanic  asian  white  \\\n",
       "54066  150.0  2.77       1.0     1.0    0.0       0.0    0.0    1.0   \n",
       "59441  155.0  2.81       0.0     1.0    0.0       0.0    1.0    0.0   \n",
       "11354  161.0  3.78       0.0     1.0    0.0       0.0    0.0    1.0   \n",
       "68562  164.0  3.44       0.0     1.0    0.0       1.0    0.0    0.0   \n",
       "97741  149.0  3.79       0.0     0.0    0.0       0.0    0.0    1.0   \n",
       "...      ...   ...       ...     ...    ...       ...    ...    ...   \n",
       "51882  153.0  3.05       1.0     1.0    0.0       0.0    0.0    1.0   \n",
       "62536  162.0  3.57       0.0     0.0    0.0       0.0    0.0    1.0   \n",
       "98995  156.0  3.30       1.0     1.0    0.0       0.0    0.0    1.0   \n",
       "6304   152.0  3.22       1.0     1.0    0.0       0.0    0.0    1.0   \n",
       "62481  153.0  3.11       0.0     1.0    0.0       0.0    0.0    1.0   \n",
       "\n",
       "       other_race  c_admit_rate  c_enroll_rate     es  admit  enroll    p_i  \\\n",
       "54066         0.0         0.649          0.381  0.323    0.0     0.0  0.110   \n",
       "59441         0.0         0.000          0.285  0.151    0.0     0.0  0.010   \n",
       "11354         0.0         0.219          0.180  0.108    1.0     0.0  0.500   \n",
       "68562         0.0         0.361          0.149  0.387    1.0     0.0  0.590   \n",
       "97741         0.0         0.642          0.405  0.280    0.0     0.0  0.090   \n",
       "...           ...           ...            ...    ...    ...     ...    ...   \n",
       "51882         0.0         0.649          0.381  0.323    0.0     0.0  0.070   \n",
       "62536         0.0         0.000          0.285  0.151    1.0     1.0  0.608   \n",
       "98995         0.0         0.638          0.740  0.500    1.0     1.0  0.960   \n",
       "6304          0.0         0.693          1.000  0.301    0.0     0.0  0.010   \n",
       "62481         0.0         0.000          0.285  0.151    0.0     0.0  0.000   \n",
       "\n",
       "        q_i  \n",
       "54066  0.00  \n",
       "59441  0.00  \n",
       "11354  0.59  \n",
       "68562  0.04  \n",
       "97741  0.00  \n",
       "...     ...  \n",
       "51882  0.00  \n",
       "62536  0.37  \n",
       "98995  0.97  \n",
       "6304   0.00  \n",
       "62481  0.00  \n",
       "\n",
       "[22754 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lg_clf.predict_proba(X_test)\n",
    "X_test = pd.merge(X_test, y_test, left_index = True, right_index = True)\n",
    "X_test[['p_i', 'q_i']] = np.round(y_pred.toarray(), 3)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a7425",
   "metadata": {},
   "source": [
    "### Note on the selection of candidates for a given department"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324bf0d9",
   "metadata": {},
   "source": [
    "In APD-S, since we focus on one academic unit (e.g, department), we would only select applicants from colleges with similar admit rate and enroll rate (this means that we can form 1 department out of those schools) for the input instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2de79bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22754.000000\n",
       "mean         0.347415\n",
       "std          0.238890\n",
       "min          0.000000\n",
       "25%          0.192000\n",
       "50%          0.315000\n",
       "75%          0.432000\n",
       "max          1.000000\n",
       "Name: c_admit_rate, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of college admit rate\n",
    "X_test['c_admit_rate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412991b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22754.000000\n",
       "mean         0.356386\n",
       "std          0.237919\n",
       "min          0.000000\n",
       "25%          0.180000\n",
       "50%          0.285000\n",
       "75%          0.433000\n",
       "max          1.000000\n",
       "Name: c_enroll_rate, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of college enroll rate\n",
    "X_test['c_enroll_rate'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee62fd",
   "metadata": {},
   "source": [
    "Based on the summary of the statistics of colleges' admit and enroll rate, we would select applicants from college in the range of 25th percentile and 75th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7de1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[(X_test['c_admit_rate'] >= np.percentile(X_test['c_admit_rate'], 25)) & \\\n",
    "                (X_test['c_admit_rate'] <= np.percentile(X_test['c_admit_rate'], 75)) & \\\n",
    "                (X_test['c_enroll_rate'] >= np.percentile(X_test['c_enroll_rate'], 25)) & \\\n",
    "                (X_test['c_enroll_rate'] <= np.percentile(X_test['c_enroll_rate'], 75))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622475c5",
   "metadata": {},
   "source": [
    "# Generate input instance for APD-S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171608f9",
   "metadata": {},
   "source": [
    "### Specification\n",
    "An input instance of APD can be characterized as $I = ([n], \\{p_i, q_i |i \\in [n]\\}, \\{B_g, g|g \\in G_I\\}, \\{b_g, g|g \\in G_E\\}, \\{w_{ig}, \\tau_g|g \\in G_P, i \\in g\\})$.\n",
    "\n",
    "**Note:** \n",
    "- In this dataset, we assume all candidates are qualified for an interview and they will be automatically offered once passing it. Hence, it can be explained why we use the result of *admit* as *passing the interview* and *enroll* as *accepting the offer*.\n",
    "\n",
    "**Input details:**\n",
    "- [n] = {1000, 2000, 3000, 4000, 5000}\n",
    "- $p_i, q_i$ for each i $\\in$ [n]\n",
    "- $G_I$:\n",
    "    - In APD-S, we only have single interview-related constraint that can be captured as {g, B} with g = [n] being the only group in $G_I$. So $G_I$ contains only 1 group with n candidates.\n",
    "    - In reality, the dataset should miss a lot of candidates that fail to get an interview since it only includes those who qualify for an interview. In this case, we can simulate how colleges with similar acceptance rate (~20-30%) as schools in this data perform. We found [Admissions report of Oxford Law](https://www.law.ox.ac.uk/sites/files/oxlaw/ug_admissions_report_2021.pdf) and identified that we can use its application-to-interview success rate to set the cap on interview-related group g $B_g$ accordingly.\n",
    "- $G_E$:\n",
    "    - We have enrollment-related budget constraints {g, $b_g|g \\in G_E$}. In this dataset, there are two groups inside $G_E$ which indicate candidates who are in-state and those who are out-of-state.\n",
    "    - Since the data should not miss any candidate who successfully enrolls, we can set the cap on enrollment-related group g $b_g$ as the actual statistics of the dataset (of those who enroll, who are in-state applicants, who are out-of-state applicants?)\n",
    "- $G_P$:\n",
    "    - We capture the collection of protected groups of interest $G_P$ as the combination of the race and gender of the candidates. That means $G_P$ will have 8 groups by combining Race: {Black, Hispanic, Asian, White} and Gender: {Male, Female}.\n",
    "    - We identify the target quota $\\tau_g$ for protected group g using the admission statistics of universities that are known for applying Affirmative Action in their admission process. \n",
    "        + Specifically, we would use the statistics of Harvard Law School, known for its [yearly commitment to Affirmative Action in the admission/employment process](https://hr.harvard.edu/files/humanresources/files/reaffirmation_statement.pdf)\n",
    "        + Based on the [demographics of Hardvard Fall 2020 applications](https://www.ilrg.com/rankings/law/view/49), we calculate the percentage of each protected group in the enrollment number and set $\\tau_g$ accordingly. \n",
    "    - For each candidate i $\\in g$ of $G_P, w_{ig}$ (the degree of relevance of i to g) is calculated by the economic status of the candidate. We use the school's location to reflect the poverty rate - an important indicator for economic status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a40d26",
   "metadata": {},
   "source": [
    "### Define n candidates and randomly sample a data of size n from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d704dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 random input instances from the testing data set, each with {1000, 2000, 3000, 4000, 5000} candidates\n",
    "sizes = np.array([n * 1000 for n in range(1, 6)])\n",
    "input_instances = np.array([X_test.sample(n = n).reset_index(drop = True) for n in sizes], dtype = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80114771",
   "metadata": {},
   "source": [
    "### Function to generate input instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b68a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_input(data):\n",
    "    # collection of interview-related groups\n",
    "    G_I = data\n",
    "    \n",
    "    # collection of enrollment-related groups\n",
    "    in_state = (data['resident'] == 1)\n",
    "    out_of_state = (data['resident'] == 0)\n",
    "    G_E = np.array([data[in_state], data[out_of_state]], dtype = object)\n",
    "    \n",
    "    # collection of protected groups\n",
    "    female_black = data[(data['gender'] == 0) & (data['black']==1)]\n",
    "    female_hispanic = data[(data['gender'] == 0) & (data['hispanic'] == 1)]\n",
    "    female_asian = data[(data['gender'] == 0) & (data['asian'] == 1)]\n",
    "    female_white = data[(data['gender'] == 0) & (data['white'] == 1)]\n",
    "    female_other = data[(data['gender'] == 0) & (data['other_race'] == 1)]\n",
    "    \n",
    "    male_black = data[(data['gender'] == 1) & (data['black'] == 1)]\n",
    "    male_hispanic = data[(data['gender'] == 1) & (data['hispanic'] == 1)]\n",
    "    male_asian = data[(data['gender'] == 1) & (data['asian'] == 1)]\n",
    "    male_white = data[(data['gender'] == 1) & (data['white'] == 1)]\n",
    "    male_other = data[(data['gender'] == 1) & (data['other_race'] == 1)]\n",
    "\n",
    "    G_P = np.array([female_black, female_hispanic, female_asian, female_white, female_other,\\\n",
    "           male_black, male_hispanic, male_asian, male_white, male_other], dtype = object) \n",
    "    \n",
    "    # cap imposed on interview-related group g\n",
    "    B_g = int(len(data) * 0.3705)\n",
    "    \n",
    "    # cap imposed on enrollment-related group g\n",
    "    b_g = np.array([len(data[in_state & (data['enroll'] == 1)]), len(data[out_of_state & (data['enroll'] == 1)])])\n",
    "    \n",
    "    # target quota for protected group g to achieve\n",
    "    target_quota = [0.0345, 0.0415, 0.0535, 0.252, 0.1185] * 2\n",
    "    tau_g = np.array(np.round([len(data[data['enroll'] == 1]) * quota for quota in target_quota]), int)\n",
    "    \n",
    "    # if there is a difference in the sum of G_P and G_E due to rounding, randomly increase one group\n",
    "    # in either G_P or G_E to balance the difference (as both cap on final enrollment)\n",
    "    if sum(tau_g) > sum(b_g):\n",
    "        index = np.random.randint(0, len(b_g)) \n",
    "        b_g[index] += sum(tau_g) - sum(b_g)\n",
    "    elif sum(tau_g) < sum(b_g):\n",
    "            index = np.random.randint(0, len(tau_g))\n",
    "            tau_g[index] += sum(b_g) - sum(tau_g)\n",
    "    \n",
    "    # relevance of i to protected group g\n",
    "    w_ig = []\n",
    "    for index_g in range(len(G_P)):\n",
    "        g = G_P[index_g]\n",
    "        arr = []\n",
    "        for index_i in range(len(g)):\n",
    "            arr.append(g.iloc[index_i]['es'])\n",
    "        w_ig.append(arr)\n",
    "    w_ig = np.asarray(w_ig, dtype = object)\n",
    "    \n",
    "    return G_I, G_E, G_P, B_g, b_g, tau_g, w_ig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a5f68",
   "metadata": {},
   "source": [
    "# Solve Linear Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2893b1",
   "metadata": {},
   "source": [
    "### Justification for solving the maximin LP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b36a63",
   "metadata": {},
   "source": [
    "The objective model is max min$_{g \\in G_P} (\\sum_{i \\in g} w_{ig} y_i q_i / \\tau_g)$.\n",
    "\n",
    "We can rewrite it as the following to solve:\n",
    "\n",
    "max z\n",
    "\n",
    "s.t $ \\space \\space$  z $\\le \\sum_{i \\in g} w_{ig} y_i q_i / \\tau_g \\space \\space \\space \\space \\space \\space$ for $g \\in G_P$\n",
    "\n",
    "Other constraints will be kept as original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d68b2",
   "metadata": {},
   "source": [
    "### Function to solve LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "360b0906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def solveLP(data, n, G_I, G_E, G_P, B_g, b_g, tau_g, w_ig):\n",
    "    # create model\n",
    "    model = LpProblem(name='APD-S', sense = LpMaximize)\n",
    "\n",
    "    # define decision variables\n",
    "    x = np.array([LpVariable('x' + str(i), lowBound = 0, upBound = 1) for i in range(n)])\n",
    "    y = np.array([LpVariable('y' + str(i), lowBound = 0, upBound = 1) for i in range(n)])\n",
    "    z = LpVariable(name='z')\n",
    "\n",
    "    # add objective function to the model\n",
    "    model += z\n",
    "\n",
    "    # constraints for z\n",
    "    for index_g in range(len(G_P)):\n",
    "        constraint = []\n",
    "        g = G_P[index_g]\n",
    "        for index_i in range(len(g)):\n",
    "            constraint.append(w_ig[index_g][index_i]*y[g.index[index_i]]*g.iloc[index_i]['q_i']/tau_g[index_g])\n",
    "        model += z <= lpSum(constraint)\n",
    "    \n",
    "    # constraints for (2) & (3) in LP since G_I = [n]\n",
    "    constraint = []\n",
    "    for i in range(n):\n",
    "        # (3)\n",
    "        model += y[i] <= x[i]*data.iloc[i]['p_i']\n",
    "        # (2)\n",
    "        constraint.append(x[i])  \n",
    "    model += lpSum(constraint) <= B_g\n",
    "        \n",
    "    # constraints for (4) in LP\n",
    "    for index_g in range(len(G_E)):\n",
    "        g = G_E[index_g]\n",
    "        constraint = []\n",
    "        for index_i in range(len(g)):\n",
    "            constraint.append(y[g.index[index_i]]*g.iloc[index_i]['q_i'])\n",
    "        model += lpSum(constraint) <= b_g[index_g]\n",
    "    \n",
    "    # solve the model \n",
    "    model.solve(PULP_CBC_CMD(msg=0))\n",
    "    \n",
    "    x_optimal = np.zeros(n)\n",
    "    y_optimal = np.zeros(n)\n",
    "    \n",
    "    for var in model.variables()[:n]:\n",
    "        x_optimal[int(str(var.name)[1:])] = round(var.varValue, 3)\n",
    "    \n",
    "    for var in model.variables()[n:-1]:\n",
    "        y_optimal[int(str(var.name)[1:])] = round(var.varValue, 3)\n",
    "    \n",
    "    # return x*_i, y*_i for each candidate i\n",
    "    return x_optimal, y_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76db50",
   "metadata": {},
   "source": [
    "# Implementation of algorithm 1 for the special case of APD-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628a65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_1(data, n, G_E, B_g, b_g, x_optimal, y_optimal):\n",
    "    # remarks on RP for APD-S\n",
    "    L_i = np.empty(n, dtype = object)\n",
    "    T_l = np.empty([n, B_g])\n",
    "    T_l_prime = np.empty([n, B_g])\n",
    "    for i in range(n):\n",
    "        # generate interval L_i for each i\n",
    "        left = sum(x_optimal[:i])\n",
    "        right = sum(x_optimal[:(i+1)])\n",
    "        L_i[i] = pd.Interval(left = left, right = right, closed = 'both')\n",
    "\n",
    "        # generate B i.i.d T_l and T'_l for each l\n",
    "        l = i\n",
    "        T_l[l] = np.random.uniform(0, np.nextafter(1, np.inf), B_g)\n",
    "        T_l_prime[l] = np.array([l - 1 + T for T in T_l[l]])\n",
    "\n",
    "    # set I_i = 1 if there exists at least one l in n with T'_l in L_i\n",
    "    I_i = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        for l in range(n):\n",
    "            if all(num in L_i[i] for num in T_l_prime[l]): # if T'_l in L_i\n",
    "                I_i[i] = 1\n",
    "                break\n",
    "\n",
    "    # step (5) - (11):\n",
    "    pi = np.random.permutation(n)\n",
    "    Z_i = np.zeros(n)\n",
    "    sum_Zi = [0, 0]\n",
    "    for l in range(n):\n",
    "        i = pi[l]\n",
    "        if I_i[i] == 1: # give interview to i\n",
    "            p_i = data.iloc[i]['p_i']\n",
    "            if np.random.choice([True, False], size = 1, p = [p_i, 1 - p_i]): # if i passes the interview\n",
    "                O_i =  np.random.binomial(1, y_optimal[i]/(x_optimal[i] * p_i))\n",
    "                if O_i == 1: \n",
    "                    for index_g in range(len(G_E)):\n",
    "                        # locate i in which group g of G_E\n",
    "                        if i in G_E[index_g].index: \n",
    "                            if sum_Zi[index_g] <= b_g[index_g]:\n",
    "                                sum_Zi[index_g] += 1\n",
    "                                Z_i[i] = 1         \n",
    "                            break\n",
    "    \n",
    "    return Z_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e9c58",
   "metadata": {},
   "source": [
    "# Verify LP constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b339d2c",
   "metadata": {},
   "source": [
    "### Function to verify the constraints of LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a930658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if x* and y* of each input instance satisfy the constraints \n",
    "def verify_LP(data, n, G_E, B_g, b_g, x_optimal, y_optimal):\n",
    "    print('Verify input instance with n =', n)\n",
    "\n",
    "    # constraint 2\n",
    "    if sum(x_optimal) > B_g:\n",
    "        print('Not satisfied!')\n",
    "        return False\n",
    "\n",
    "    # constraint 3\n",
    "    for i in range(n):\n",
    "        if y_optimal[i] > x_optimal[i]*data.iloc[i]['p_i']:\n",
    "            print('Not satisfied!')\n",
    "            return False\n",
    "\n",
    "    # constraint 4\n",
    "    for index_g in range(len(G_E)):\n",
    "        g = G_E[index_g]\n",
    "        for index_i in range(len(g)):\n",
    "            if y_optimal[g.index[index_i]]*g.iloc[index_i]['q_i'] > b_g[index_g]:\n",
    "                print('Not satisfied!')\n",
    "                return False\n",
    "            \n",
    "    # if no constraint is violated \n",
    "    print('-> Every constraints has been satisfied!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1026e58",
   "metadata": {},
   "source": [
    "### Verify LP with all input instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27a1692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify input instance with n = 1000\n",
      "-> Every constraints has been satisfied!\n",
      "Verify input instance with n = 2000\n",
      "-> Every constraints has been satisfied!\n",
      "Verify input instance with n = 3000\n",
      "-> Every constraints has been satisfied!\n",
      "Verify input instance with n = 4000\n",
      "-> Every constraints has been satisfied!\n",
      "Verify input instance with n = 5000\n",
      "-> Every constraints has been satisfied!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(input_instances)):\n",
    "    # generate input instance\n",
    "    data = input_instances[i]\n",
    "    n = sizes[i]\n",
    "    G_I, G_E, G_P, B_g, b_g, tau_g, w_ig = generate_input(data)\n",
    "              \n",
    "    # output x*_i, y*_i for each i in [n]\n",
    "    x_optimal, y_optimal = solveLP(data, n, G_I, G_E, G_P, B_g, b_g, tau_g, w_ig)\n",
    "    \n",
    "    verify_LP(data, n, G_E, B_g, b_g, x_optimal, y_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f193e",
   "metadata": {},
   "source": [
    "### Verify Lemma 2: $E[Z_i] \\ge (y_i^{*}.q_i)/2$ for every $i \\in n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c2540",
   "metadata": {},
   "source": [
    "### Function to verify Lemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e17cc2f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def verify_lemma2(data, n, G_E, B_g, b_g, x_optimal, y_optimal):\n",
    "    print('Verify Lemma 2 with input instance that has n =', n)\n",
    "    expected_Z = np.zeros(n)\n",
    "    for i in range(200):\n",
    "        Z_i = algorithm_1(data, n, G_E, B_g, b_g, x_optimal, y_optimal)\n",
    "        expected_Z += Z_i\n",
    "    expected_Z /= 200\n",
    "    flag = True\n",
    "    for i in range(n):\n",
    "        if expected_Z[i] < (y_optimal[i]*data.iloc[i]['q_i'])/2:\n",
    "            print('Error:')\n",
    "            print('E[Z_i] =', expected_Z[i])\n",
    "            print('(y*_i * q_i)/2 =', (y_optimal[i]*data.iloc[i]['q_i'])/2)\n",
    "            flag = False\n",
    "    if flag:\n",
    "        print('Lemma 2 has been verified!')\n",
    "    else:\n",
    "        print('Lemma 2 has not been verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e3e6aa",
   "metadata": {},
   "source": [
    "### Verify Lemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fdaf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify Lemma 2 with input instance that has n = 1000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(input_instances)):\n",
    "    # generate input instance\n",
    "    data = input_instances[i]\n",
    "    n = sizes[i]\n",
    "    G_I, G_E, G_P, B_g, b_g, tau_g, w_ig = generate_input(data)\n",
    "              \n",
    "    # output x*_i, y*_i for each i in [n]\n",
    "    x_optimal, y_optimal = solveLP(data, n, G_I, G_E, G_P, B_g, b_g, tau_g, w_ig)\n",
    "              \n",
    "    verify_lemma2(data, n, G_E, B_g, b_g, x_optimal, y_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b8eab4",
   "metadata": {},
   "source": [
    "# Output Approximation Ratio for RP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63454d9a",
   "metadata": {},
   "source": [
    "# Greedy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4f8d1",
   "metadata": {},
   "source": [
    "# Uniform algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffb0c1",
   "metadata": {},
   "source": [
    "# Visualize the confidence interval of the Approximation Ratio for each algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
